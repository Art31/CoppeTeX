%%
%% This is file `example.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% coppe.dtx  (with options: `example')
%% 
%% This is a sample monograph which illustrates the use of `coppe' document
%% class and `coppe-unsrt' BibTeX style.
%% 
%% \CheckSum{1613}
%% \CharacterTable
%%  {Upper-case    \A\B\C\D\E\F\G\H\I\J\K\L\M\N\O\P\Q\R\S\T\U\V\W\X\Y\Z
%%   Lower-case    \a\b\c\d\e\f\g\h\i\j\k\l\m\n\o\p\q\r\s\t\u\v\w\x\y\z
%%   Digits        \0\1\2\3\4\5\6\7\8\9
%%   Exclamation   \!     Double quote  \"     Hash (number) \#
%%   Dollar        \$     Percent       \%     Ampersand     \&
%%   Acute accent  \'     Left paren    \(     Right paren   \)
%%   Asterisk      \*     Plus          \+     Comma         \,
%%   Minus         \-     Point         \.     Solidus       \/
%%   Colon         \:     Semicolon     \;     Less than     \<
%%   Equals        \=     Greater than  \>     Question mark \?
%%   Commercial at \@     Left bracket  \[     Backslash     \\
%%   Right bracket \]     Circumflex    \^     Underscore    \_
%%   Grave accent  \`     Left brace    \{     Vertical bar  \|
%%   Right brace   \}     Tilde         \~}
%%
% useful link: https://apgita.org.br/academico/teses-e-latex/
\documentclass[msc,numbers,english]{coppe} % maybe mscexam?
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{array}

\makelosymbols
\makeloabbreviations

\begin{document}
  \title{Tradução de linguagem automática em português brasileiro através de redes neurais em domínios de baixo recurso}
  \foreigntitle{Tackling low resource neural machine translation applied to Brazilian Portuguese}
  \author{Arthur}{Telles Estrella}
  \advisor{Prof.}{João}{Baptista de Oliveira e Souza Filho}{D.Sc.}
  % \advisor{Prof.}{Nome do Segundo Orientador}{Sobrenome}{Ph.D.}
  % \advisor{Prof.}{Nome do Terceiro Orientador}{Sobrenome}{D.Sc.}

  \examiner{Prof.}{Nome do Primeiro Examinador Sobrenome}{D.Sc.}
  \examiner{Prof.}{Nome do Segundo Examinador Sobrenome}{Ph.D.}
  \examiner{Prof.}{Nome do Terceiro Examinador Sobrenome}{D.Sc.}
  \examiner{Prof.}{Nome do Quarto Examinador Sobrenome}{Ph.D.}
  \examiner{Prof.}{Nome do Quinto Examinador Sobrenome}{Ph.D.}
  \department{PEE}
  \date{02}{2021}

  \keyword{Primeira palavra-chave}
  \keyword{Segunda palavra-chave}
  \keyword{Terceira palavra-chave}

  \maketitle

  \frontmatter
  \dedication{A algu\'em cujo valor \'e digno desta dedicat\'oria.}

  \chapter*{Agradecimentos}

  Gostaria de agradecer a todos.

  \begin{abstract}

  Apresenta-se nesta dissertação um estudo dedicado a lidar com a tarefa de tradução usando redes neurais, em condições de pouca disponibilidade de dados e com apenas uma GPU, com foco específico para o português-inglês. 
  Será avaliado o efeito prático de técnicas disponíveis na literatura que possuem algum potencial de melhorar a performance nesse contexto, como subword embeddings, pre-trained word embeddings e back translation, e como elas  impactam qualitativamente no desempenho em frases de diferentes níveis de complexidade.
  Essas técnicas terão seus prós e contras avaliados e discutidos, utilizando as principais arquiteturas utilizadas na literatura, redes neurais recorrentes e baseadas em transformers.
  (avaliar isso) O melhor modelo desenvolvido é capaz de atingir x\% de score BLEU e y de perplexidade no conjunto de teste do dataset xpto.

  \end{abstract}

  \begin{foreignabstract}

  In this work, a dedicated analysis is executed to get best practices on how to tackle neural machine translation under low data availability and using a single GPU, focusing specifically on the Portuguese-English pair. 
  Techniques in the literature that can potentially boost the performance under this context will be evaluated, such as subword embeddings, pre-trained word embeddings and back translation, and the qualitative impact of them is presented in sentences with a complexity drill down.
  The tradeoffs of these techniques are discussed, contemplating the main architectures used in the literature, neural recurrent models and transformer-based ones.
  (evaluate this) The best model built is capable of reaching x\% BLEU score and y Perplexity in the test set for the xpto Portuguese dataset.

  \end{foreignabstract}

  \tableofcontents
  \listoffigures
  \listoftables
  %% --- UNDERSTAND HOW TO USE THIS ---
  \printlosymbols
  \printloabbreviations
  %% ----------------------------------
  \mainmatter
  \chapter{Introduction}
  \section{The reenactment of machine translation}
  
  The machine translation is a research field that until 2013 has mainly invested in statistical based models, but the breakthrough promoted by sequence to sequence algorithms followed by the use of transformer models has significantly changed the focus of the field. Before neural networks, machine translation systems were rules-based, syntax-based, phrase-based or a blend between more than one of these techniques. Probabilistic models were used and considered state of the art just before the first sequence to sequence paper appeared. The increase in performance promoted by the sequence to sequence and transformers quickly received some attention, and soon other variants were developed.
  
  Despite being constrained by computational power in many stages since its beginning, one of the most relevant contributions to the translation task were the transformers, which made possible to perform the computation in a truly parallel schema. With this new architecture, the operations performed during training are not totally dependent, allowing them to become parallelizable in the GPU. By removing the constraint of some operations having to wait for others to finish, NMT models were enabled to scale and reach even higher quality translations.
  
  In 3 years NMT became the dominant approach to machine translation, inducing a major transition from statistical to neural models. An interesting scenario reached the interest of researchers to explore this approach: the broad set of parameters and architectures already present in the neural networks literature that could be used as a toolkit to boost translation quality and the promising results being presented at the time.

  \section{Challenges for the Portuguese language}

  Traditionally, the machine translation datasets and conferences usually focus on a subset of languages from countries that are actively investing on NLP, which biases and narrows the potential that the algorithms have towards a specific domain. Unfortunately, Portuguese is a language that does not ostentate supervised translation data in diversity and quantity, an issue that increases the struggle to build a model that can successfully translate it to other languages. Another obstacle is that Portuguese has european, brazilian and african variants, this provides a challenge for a model since generalization is harder if several sentences with different dialects can have the same meaning.
  
  The branch of NMT inside natural language processing is also a field with few papers and academic works among Brazilian universities, this can be partially explained by the challenge that this environment presents: most models require cutting edge GPUs and usually only one GPU is not enough for medium sized model on an average WMT competition dataset. The scarcity of these resources for research purposes require students to innovate in a limited domain and search for cloud solutions without sponsorship.
  
  Finally, Portuguese is a complex language that uses accents which can change their meaning (i.e. "e" and "é"), has different pronoun organizations (i.e. "realizar-se-á" equals "se realizará") and irregular verb inflections (i.e. the "pôr" and "haver" verbs) so text preprocessing and tokenization plays an important role. Disregarding these details by applying some generic preprocessing steps that eliminates accents for instance can lead to worse model performance. On the other hand, having some domain knowledge and apply this to the NLP pipeline can help the model better translate or classify, depending on the desired task.
  
  \section{Contributions of this dissertation}
  
  The generic contribution of this work is the evaluation of a set of techniques available in the literature for NMT that can help dealing with low resource domains, although being applied to Portuguese they can be generalized to other languages. There is also a specialized contribution under a qualitative domain: performance gains provided by these techniques are evaluated in a subset of sentences that contain a complexity drill down. This enables the reader to have a deeper understanding of their effects on the Portuguese language.

  \section{Chapter Organization}

  \textcolor{red}{(a ideia final para a dissertação do chapter 2 é falar de arquitetura e revisão teórica/bibliográfica de fundamentos da arquitetura, depois no 3 mostrar restrições que acontecem em low resource domains e suas consequencias, com revisão bibliográfica das técnicas disponíveis que podem ajudar nesse contexto e a motivação das escolhas feitas)}

  An explanation of the transformer and recurrent neural network architectures and how researchers iterated on them to become state of the art is given in chapter 2.
  
  In chapter 3 constraints that arise in low resource domains and a review of techniques that can potentially help to reduce those issues are presented, along with the rationale behind our choices.
  
  The datasets chosen are presented in chapter 4, where we also describe implementation details, how hyperparameters were iterated on to fit in a single machine GPU and results of the experiments, which are measured in terms of validation perplexity, BLEU and \textcolor{red}{TER(?)(<to be defined>)}. A qualitative discussion regarding human and model translations in different complexity levels is also performed.
  
  \textcolor{red}{(Finally, in chapter 5 this work is concluded and further improvements and study directions are outlined.)}
  
  Finally, in chapter 5, next steps to conclude this work in the following months are presented

  \chapter{Neural networks and machine translation}

  Since their ideation in 1958, neural networks have seen peaks and valleys of research focus in many different artificial intelligence fields. After going out of the tar pit in the 1980s with feedforward and recurrent variants, receiving convolutional and LSTM variants in 1990s, they started to leverage promising results when applied to numerical and categorical data. Specially after 2013 neural networks were growing at an unseen rate, applications to image processing and natural language processing were massively explored and soon consolidated themselves as state of the art algorithms. 
  
  Among the first relevant contributions coming from neural networks to the NLP domain was \citet{mikolov2013efficient}, that managed to execute the task of efficiently adjusting word vectors in a N dimensional space using a 2 layer shallow neural network. This was accomplished by adjusting word vectors based on the likelihood of a word to occur given a context or vice versa.
  
  For a long time, this paradigm hasn't seen any applications to solve translation tasks, until 2013 when \citet{cho-etal-2014-learning} came up with a RNN Encoder-Decoder architecture proposal. The focus of the scientific community at the time that was on statistical models, but as the application of recurrent networks to this domain gained maturity, the papers gradually started to switch their focus.
  
  \section{Recurrent neural networks as machine translators}

  In Cho et al's approach, one RNN reads each symbol of an input sequence x sequentially and encodes this sequence of symbols into a fixed-length vector representation $c$, the hidden state. The other decodes this representation into another sequence of symbols by predicting the next symbol $y_{t}$ given the hidden state $h_{{t}}$. The equation for the decoder's hidden state at time $t$ is given below.

  \begin{equation}
  h_{{t}} = f(h_{t-1}, y_{t-1}, c)
  \end{equation}
  
  In this setup, the encoder and decoder of the proposed model are jointly trained to maximize the conditional log-likelihood of a target sequence given a source sequence. It has the drawback of losing some of the information provided by the encoder, since only the hidden state is used, and the RNN output is discarded. Another issue is that the neural network has to compress all the necessary information of a source sentence into a fixed-length vector, which provides a challenge to deal with long sentences.

  \subsection{Bahdanau's attention mechanism}

  To address the fixed-length and "information compression" loss issues identified in the previous implementation, the authors of the first paper proposed another way of connecting the encoder to the decoder \citet{bahdanau2016neural}, where the former encodes the input sequence into a sequence of vectors and extracts a subset of these vectors (not a single fixed-length one), while decoding the translation.
  
  The encoder output (also called context vector) $c_{t}$ is now a variable length vector that represents annotations and comprises a mechanism that extract information from different parts of the input sentence. Each annotation receives a weight and the final vector built can be seen as a weighted average of the inputs, where these weights represent a way to distribute attention. 
  
  This attention mechanism can be also interpreted as an alignment model, that directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly, and this new information flow partially explains the performance gains reported.
  
  Another improvement that contributed to the success of this approach is the use of bidirectional RNNs to encode the input sequence, the vector that flows from the encoder to the decoder now contains a representation not only of the preceding words but also of the following words, allowing a more efficient attention weight distribution. Despite these improvements, the attention mechanism is still rather simple and does not accomplish the importance attribution task as well as its succeeding variants, implemented in other papers later on.
  
  \textcolor{red}{(equations from the paper will be put in here later to matematically explain how attention is distributed)}
  
  An interesting work that explores the potential the previous presented RNN models have is executed by \citet{britz2017massive}, where using \cite{cho-etal-2014-learning} as a base model, several hyperparameters like embedding size, RNN cell variant (LSTM and GRU), encoder and decoder depth, unidirectional vs. bidirectional RNN encoders, attention mechanism and beam search parameters are iterated and findings about the best setups are reported. One of the claims of this work is that careful hyperparamter tuning can yield better results than many others that explored architecture variations.
  
  Among interesting results reported on \citet{britz2017massive} are that larger embeddings consistently outperforms smaller ones by a thin margin, LSTM cells consistently outperformes GRU cells, deeper decoders tend to lead to performance increases, additive attention achieves slightly better results than multiplicative and beam searches tuned to a "sweet spot" can increase model performance up to 5\%. Some of these conclusions are incorporated into our hyperparameters also, but given the low resource domain constraints and other features to iterate, our hyperparameter search is a bit narrower.

  \section{Transformer-based models}
  
  A relevant contribution to a field that at the time was mainly iterating in recurrent and convolution based neural networks for machine translation was made in 2017 by \citet{DBLP:journals/corr/VaswaniSPUJGKP17}. This new proposed family of transformer models is not constrained by the same condition as RNNs, which as a consequence of its sequential pattern, has to wait for an operation to finish to begin another one. This new approach gained traction and adherece from the community since parallelization was a strength, along with it came the potential to explore possible variations.
  
  In the original implementation, the encoder and the decoder contain 6 stacked transformer layers. In the encoder, each layer contains a multi-head self-attention mechanism, followed by layer normalization and a feed-forward layer. The structure in the decoder is quite similar, although an extra masked multi-head attention layer is added over the output of the encoder stack before multi-head attention. Backpropagation operates end-to-end in this architecture.
  
  The attention mechanism differs from previous implementations, but the goal is the same: to enable the model to distribute the influences of every input on the outputs generated. It can be described as mapping a query and a set of key-value pairs to an output, where the query (Q), keys (K) and values (V) are all vectors with dimensions defined by a hyperparameter. The original paper implements scaled dot product attention, computing dot products of the queries with all keys, then dividing by a normalizing factor $\sqrt{d_{k}}$ and passing this result into a softmax.
  
  \textcolor{red}{(equations from the paper will be put in here later to matematically explain attention)}
  
  Multi-head self-attention also plays a role on the contribution given by the authors, they claim that it is beneficial to project the matrices Q, K and V in attention h times, with different learned projections by each one of the heads. The projections of each head are concatenated and the attention weights now are adjusted to different representation subspaces at different positions, which at the beginning was thought to be consistently contributing to performance. Years later after an analysis performed by \citet{DBLP:journals/corr/abs-1905-09418}, this hypothesis was rejected. 
  
  The authors evaluated how the contribution of each attention head happened during the training process, and they find that only a small subset of heads are enough to sustain the transformer translation scores. They analyse the importance distribution and discover that several heads learn similar dependency mappings. By using this map as a qualitative signal of feature importance per head, they propose to prune the heads that have similar dependency maps and conclude that there is no noticeable loss in translation quality by doing this. 
  
  This paper raises a relevant question to the community: are deep learning works usually biased to increasing complexity and model parameters unnecessarily? Efficient models have a smaller carbon footprint and energy waste, which can lower the impact on nature, this could bring the community closer to the green AI status and increase credibility of deep learning research.

  \section{Attention variants for sequence to sequence learning}
  
  Not only architecture related parameters can lead the transformer to better quality translations, the attention mechanism also has attracted the interest of some contributions. Despite the evidence provided that increasing the number of heads in attention mechanisms may generate redundancy, there are works that report better results going against this advice by creating multiple attention branches, where each branch is an independent multi-head attention mechanism \cite{DBLP:journals/corr/abs-2006-10270}. 
  
  In this paper, other techniques such as a drop branch mechanism (similar as dropout, but applied to branches) and a specific initialization recipe for each branch may have an effect on the final outcome, so it is not clear whether the benefits come from multiplying attention or from those techniques.
  
  Iterations on the set of words that attention sees and on the weight distribution function are among the most common variants. In the paper published by \citet{luong-etal-2015-effective}, the attention mechanism is split into 2 classes: global and local attention. The former always attends to all source words of a given sentence, while the latter only looks at a subset of words at a given time. 
  
  Local attention is explored with different alignments, monotonic and predictive. In the first case, they just assume that source and target sequences are roughly monotonically aligned and concatenate the current hidden state with the source hidden state. For the predictive variant, they use a gaussian distribution centered around the word to be predicted and define alignment weights based on this distribution, adjusting the variation behaviour of weights to follow a normal distribution.
  
  The main contributions are that attention-based models usually outperform non-attentional ones and that a small increase in performance can be observed by adding a custom weight function (gaussian-based in this case). Also, using an ensemble of attention architectures may be also effective as they claim new state of the art results by doing so, even though further details of this ensemble are not provided.
  
  Integrating attention mechanisms with different attention perspectives is another exploited approach, which has been followed by \citet{calixto-etal-2017-doubly} and \citet{cui-etal-2019-mixed}. In the former, a single layer feedforward network is used to compute the expected alignment between each annotation vector in both mechanisms, but this increases the amount of parameters to train the model. In the latter, the concept of forward and backward attention is introduced, where specialized masks help the transformer to model word other information in a slightly different schema of the positional encoder used in the original paper. Both mechanisms are concatenated with the standard global and local attention, although this leads to better BLEU it also increases model parameters.
  
  There is no evidence that iterating on the attention mechanism would provide any benefits in low resource settings, furthermore all iterations usually increase model paremeters, hence hindering our low resource scenario. This together with the fact that data augmentation and embedding techniques have better potential, no iterations were made on the traditional attention mechanism of RNNs and transformers in this work.
  
  \section{How far transformers can go? Which changes are the most promising?}
  
  Talk briefly of GPT, maybe bert, and other transformer variations
  
  GPT-3 \citet{brown2020language}
  
  DeFINE: DEep Factorized INput Word Embeddings for Neural Sequence \citet{DBLP:journals/corr/abs-1911-12385}
  
  DeLighT: Deep and Light-weight Transformer \citet{mehta2021delight}
  
  \subsection{Alternative training objectives}
  
  Despite many state of the art models having reached their scores using the original maximum likelihood estimation learning objective, the work of \citet{ranzato-sequence} indicate two drawbacks of using it in NMT. First, during training NMT models are not exposed to their translation errors, this phenomenon is referred to as exposure bias. Finally, the MLE estimation is defined at word-level rather than sentence level, so theoretically the optimization objective is not aligned with the final objective, which is to generate a sentence that can match a human translation. 
  
  With this problem in mind, they introduce Mixed Incremental Cross Entropy Reinforce (MIXER), which switches the learning objective towards sentence-level training. This algorithm tries to handle the problem of backpropagating gradients on non-differentiable metrics such as BLEU with some ideas borrowed from reinforcement learning (RL). 
  
  Another technique proposed by \citet{shen-etal-2016-minimum} reported results that also raised more awareness to this learning objective issue. They claim that as MIXER samples only one candidate to calculate the reward while MRT generates multiple samples, this potentially increases its capability of discrimination, pretty close to the effect of increasing beam search in a standard NMT setup.
 
  Apart from the efforts put into iterating on the learning objective and benefits claimed by these papers, some side effects of switching the training objective are also diagnosed by \citet{DBLP:journals/corr/abs-1907-01752}. They point out the weakness of RL based approaches for optimization, claim that some of the increases are not fully attributed to the techniques and also discuss convergence issues of these new objectives. Given this unclear scenario of how iterating on the learning objective can lead to better translations, changing it in our work was not taken into consideration.

  \chapter{Low resource domain context}
  
  By definition, low resource situations happen when any of the resources needed to commit to one task are lacking, and in NMT this can happen in many ways. To train a supervised model, a large enough language pair parallel dataset is needed, and if the model is data-hungry, which is an issue that increases with model complexity, a small dataset can significantly hinder results. There are some situations where if a dataset is small enough, phrase based machine translation models can outperform NMT, such as the case study reported in \citet{sennrich-zhang-2019-revisiting}.
  
  The second most important matter that is fundamental to the success of a translation model is the infrastructure needed for the training setup, which requires one or more GPUs or TPUs to be trained. Datasets considered to be small (<100k sentences) often do not fit into CPU, and when they do, training is so slow that sometimes one has to wait from 4 to 12 times more than with a GPU. The bigger a dataset is, and remembering that size is affected by number of sentences and number of words per sentence, the more memory it will require from a GPU to be fit in every epoch.
  
  There are other neccessary resources such as disk storage, internet, a configured python environment (or another language of choice), but these are usually less relevant and more accessible, so usually not a blocker. When referring to low resource domain, we only consider dataset size and GPU power as mentioned above.
  
  \section{Datasets of this work}
  
  Given the goal of analysing translation quality with different sentence complexity levels, portuguese datasets were carefully chosen to match our GPU and model complexity constraints. Regarding GPU memory, from highest to lowest, the 3 hyperparameters that impact it most are batch size, hidden size and embedding size. Low batch size can significantly increase the time to train and hinder the training process, since the model sees less examples at each iteration. We had to determine a hard cutoff where if a model cannot be trained with at least that batch size, it will be discarded. That batch size is 64.
  
  Based on experimental iterations, some datasets such as europarl \cite{philipp-koehn-europarl} which contain 1.9 million sentences, even when reduced to more than 10 times its original size, still could not fit into a single Nvidia Tesla K80, T4 or P100 GPU, within our batch size threshold of 64. Attempts to do this using a traditional transformer from \citet{DBLP:journals/corr/VaswaniSPUJGKP17} and a RNN model based on \citet{bahdanau2016neural} with 25\% of the original embedding and hidden layer sizes both failed. As europarl is a domain specific dataset with long sentences, reducing its size results in expecting the model to learn long and complex sentences with less data, which is unfeasible. Therefore, this dataset was discarded from our list.
  
  Another publicly available portuguese dataset that consists of TED talks \cite{cettolo-ted-talks} represented a relevant addition to the experiments performed in this work, as it easily fitted our goals. This dataset has some nice properties, it contains mixed domains since the subject of talks can vary a lot, sentences range from easy to high complexity and it has about 220k sentences (after filtering common XML tags) for the training set, which is considered to be medium sized. As this dataset worked in our setup with the previously mentioned models and above our minimum batch size, it is included in the experiments performed.
  
  \textcolor{red}{1. seria legal disponibilizar uma replica do dataset TED talks, ou pelo menos quais linhas foram usadas do dataset original} \textcolor{red}{2. deveria colocar a métrica de palavras totais e palavras unicas pro dataset de ted talks?}
  
  There is a collaborative dataset created and maintained by the Tatoeba Project \cite{DBLP:journals/corr/abs-2010-06354} available on the internet that contains several English-Portuguese sentence pairs. The project contains datasets with several bilingual sentence pairs, considered to match an intermediate english level and translated from English to several other languages, including Portuguese. The Portuguese dataset contains 165k sentences, with 993k words and 21k unique words. It was built by native speakers and has been reviewed by a linguistic expert. This dataset easily fits into memory and provides low to medium complexity sentences to the model, so it's a good complement to TED Talks.

  \section{Addressing open vocabulary}

  NMT typically operates with fixed vocabulary, which can be expected in artificial splits using cross validation, but does not reflect what happens in real world cases. Due to practical reasons related to computational constraints, the vocabulary size of an average NMT model ranges from 30k to 50k words. Most models will have to deal with out of vocabulary (OOV) words when seeing new data, and word-level NMT usually performs poorly under these settings. A word-level model uses a unknown token "<unk>" to represent unseen words, there are some ways to avoid this, one of the most common ones are to capture embeddings in a different level. 
  
  There are other techniques that allow the NMT task to be more resilient to the open vocabulary issue that are also reported in the literature. They will be presented in the following sections.
  
  \subsection{Transfer learning}
  
   Transfer learning is a common method in machine learning where a model developed for a specific rtask is reused as the starting point for a model in another task. This technique was used by \citet{zoph-etal-2016-transfer}, where the weights of a high resource language pair are transferred to another unrelated low resource language pair, both having English as the target language. The lift in BLEU was enough to make the NMT system to overcome the previous state of the art syntax based machine translation (SBMT) system.
   
   Another approach to transfer learning that acts upon the embeddings learned is the use of pre-trained word embeddings, which are trained as language models to adjust vectors inside a N dimensional space. The learned representations (weights) are used by the NMT model as a starting point, instead of the random initialization that happens when the model is trained from scratch.
   
   An interesting work that analyses the effect of using pre-trained word embeddings in low resource scenarios is performed by \citet{qi-etal-2018-pre}. Some of the findings reported are that there is a "sweet spot" where word embeddings are the most effective, when there is little training data but not so little that the model cannot be trained and that pre-trained word embeddings seem to be more effective to more similar language pairs. 
   
   The performance increases of the work done by \cite{qi-etal-2018-pre} achieved in several setups exceeds the gains and potential for the technique presented at \cite{zoph-etal-2016-transfer}. Moreover, potential language misalignments that may arise from the latter provides enough evidence to choose pre-trained word embeddings as the preferred transfer learning rather than fine tuning from a different dataset.
  
  \subsection{Subword embeddings}
  
  This type of embedding does not suffer from the OOV issue as the traditional word-level setup, as it breaks words into smaller units and learns how to group these units to reconstruct the respective translation for the original sentence. Subword models can be designed to act in a character level or in an intermediate level between word and character (like phonemes, syllables and others). The time complexity of encoder-decoder architectures is at least linear with respect to the sequence length, despite the success this technique achieved, one of the drawbacks of the original implementation is that one cannot clearly define the maximum vocabulary size a priori. 
  
  Given the trade-offs of using subword models to address the open vocabulary issue along with the unwanted side effect of increasing vocabulary size, some research was performed towards using character-level RNNs only when word-level embeddings generate a OOV word in \citet{DBLP:journals/corr/LuongM16}. This was a recommended approach before subword started to mature, as they reduce the vocabulary size when compared to character-level
  
  One of the pioneer works to introduce subword embeddings was done by \citet{DBLP:journals/corr/SennrichHB15}, where a segmentation algorithm based on byte pair encoding (a compression algorithm) is introduced to learn segmentations based on text. One of the strengths of this method is that it is simple, cheap to run, easy to understand and effective. One hyperparameter needed to tune this algorithm is the number of merge operations, which is proportional to the final vocabulary size.
  
  A work published later by \citet{sennrich-zhang-2019-revisiting} reports the practical effect of changing word-level NMT to subword NMT using BPE, along with other techniques such as lexical model \cite{nguyen-chiang-2018-improving}. The ultra-low resource setting is the one that benefited the most, going from 7.2 BLEU to 16.6 using the base model of \citet{bahdanau2016neural}. 
  
  In order to address the a priori vocabulary issue mentioned before, Google has developed an useful toolkit that implements several sentence segmentation algorithms (with BPE being one of them) that are language agnostic \cite{DBLP:journals/corr/abs-1808-06226}. This implementation was made in C++ and is also available as a python library. It is designed to be a flexible toolkit with a fast and lightweight implementation that allows the a priori vocabulary size definition and the use of other features such as NFKC-based text normalization. Given all the pros of using this implementation compared to implementing BPE on our own, this toolkit is used to develop our subword embedding models.
  
  Pre-trained word embeddings also followed the subword trend with the work published by \citet{DBLP:journals/corr/BojanowskiGJM16}, which became popular with the name of fast text. This algorithm is inspired on word2vec using the skipgram variant, it treats words as a bag of character n-grams and adds special symbols ">" and "<" at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences. Each word is assigned a number of n-grams, and in practice only n-grams for n greater than or equal to 3 and smaller or equal to 6 are extracted for each word. Finally, a word is represented by the sum of the vector representations of its n-grams, when learned it is represented by an index and a respective set of n-grams. 
  
  \subsection{Data Augmentation}
  
  The amount of parallel data is key to the success of NMT, and data-hungry complex models provide limitations for scientists contributing to this field. Unfortunately, large-scale parrallel corpora is not available for the majority of the existing language pairs. On the other hand, it is much easier to obtain monolingual corpora, and this scenario motivated the study of data augmentation as a supportive method to NMT.
  
  Aiming to transfer the knowledge from language models to NMT, \citet{gulcere-language-model-nmt} shows 2 attempts to achieve this task: shallow fusion and deep fusion. The first uses a language model during decoding to rescore the n-base list. The other combines decoder and language model which are coordinated with a controller mechanism. Unfortunately, improvements reported are limited to less than 1 BLEU. 
  
  A branch of research studies variations of the technique referred to as back-translation (BT). In \citet{DBLP:journals/corr/SennrichHB15a} they extract target-side monolingual data, train a model to translate back to the source language and uses it to build synthetical translations, that are reused on the original dataset to train another version of the NMT model. BT has shown to be a simple, yet effective method to address low data availability in many domains, as shown in \citet{DBLP:journals/corr/abs-1804-06189}. They show that there is a trend of increasing translation performance with certain a amount of synthetic translations added, but it appears to tail off when the dataset balance is tipped too far in favour of the synthetic data. 
  
  Other ways of generating synthetic data are presented and compared in the work presented by  \citet{DBLP:journals/corr/abs-1906-03785}. Augmentation methods are discussed in scenarios with high resource languages (HRL) and low resource languages (LRL), where it can be performed via pivoting, word substitution (with the use of a dictionary) and even through unsupervised methods. They also introduce a two-step pivoting method that can use a HRL to create artificial examples for a LRL, when only a small dataset with both languages is available. This is particularly useful for languages that have even less available resources than Portuguese. 
 
  \chapter{Results}
  
  \section{Methodology}
  
  On the experiments made, embeddings such as \citet{pennington-etal-2014-glove} and \cite{DBLP:journals/corr/BojanowskiGJM16} were used, the former was chosen with the goal of benefiting from global statistic features of word and the latter because of the positive effect of subword units in addressing open vocabulary. Both embeddings were trained in a large portuguese monolingual dataset and made available by \citet{DBLP:journals/corr/abs-1708-06025}. 
  
  The preprocessing tokenization pipeline applied by them is the same used in our experiments. In summary, numbers, emails and URLs are replaced by their respective tokens, punctuations are reduced to a single character when they occur more than once, some html formatting is removed and the text is converted to lowercase. The experiments were conducted using Python, Gensim was the library used to deal with word embeddings, sacrebleu \cite{post-2018-call} is the implementation used for calculating BLEU score and language specific Spacy tokenizers were also used.
  
  The tatoeba dataset, was split into train, test and validation, with the test set having 10\% of the original dataset. The remaining data was split into train and validation sets, with 90\% and 10\% each. In order to reduce the size to fit in GPU memory, in TED talks dataset the first 1295 talks out of 1938 in the original file were selected for the training set, and the following ones were discarded. This results in a training set with 169k sentences, test with 11.4k sentences and dev 4.15k.
  
  The neural networks were trained from scratch on NVIDIA Tesla K80 and T4 GPUs, the metrics used to monitor the training progress were perplexity and loss, for both training and validation sets. Perplexity measures the inverse of the geometric average probability attributed to an occurring word, the lower it is the better the model is performing. During training, a new model is saved each time the best validation perplexity is achieved. If the validation perplexity doesn't improve for 10 epochs, the process is halted, meaning it reached convergence. This rule was applied to all the models.
  
  \section{Qualitative evaluation}
  
  \subsection{Translation metrics with complexity drill down}
  
  \begin{table}[h]
  \caption{Translation metrics with complexity drill down}
  \label{tab:citation}
  \centering
  {\footnotesize
  \begin{tabular}{| p{4cm} | p{1cm} | p{4cm} | p{4cm} | p{1cm}|}
    \hline
    Hipothesis & \verb|Complexity| & \verb|Reference| & \verb|Translation| & \verb|BLEU|\\
    \hline
    Eu voei no Força Aérea 2 por oito anos. & \verb|A1| & \verb|I flew on Air Force Two for eight years.| & \verb|I flew in Air Force Two for eight years.| & \verb|0.8|\\
    Não conseguimos produzir algum tipo de combustível mais limpo para cozinhar?. & \verb|B1| & \verb|Can't we make cleaner burning cooking fuels?| & \verb|Can we not make cleaner cooking burn fuels?| & \verb|0.6|\\
    Então isso é o que eles fazem. & \verb|A1| & \verb|I flew on Air Force Two for eight years.| & \verb|And so this is what they do.| & \verb|0.7|\\
    Mas é quase como se o corpo tivesse originado essa resposta engenhosa mas não conseguisse de fato controlá-la. & \verb|B2| & \verb|But it's almost as if the body has originated this ingenious response, but can't quite control it.| & \verb|But it's almost as if the body has originated this ingenious response, but can't quite control it.| & \verb|0.5|\\
    % Artigo & \cite{article-example} & \citet{article-example}\\
    % Relat\'orio & \cite{techreport-example} & \citet{techreport-example}\\
    % Relat\'orio & \cite{techreport-exampleIn} & \citet{techreport-exampleIn}\\
    % Anais de Congresso & \cite{inproceedings-example} &
    %   \citet{inproceedings-example}\\
    \hline
  \end{tabular}}
  \end{table}

  \chapter{Next steps for this dissertation}
  
  Publish paper at CBIC (30/05) or STIL (TBD)
  
  Preliminary deadline at 10/09/2021
  
  
  Segundo a norma de formata{\c c}\~ao de teses e disserta{\c c}\~oes do
  Instituto Alberto Luiz Coimbra de P\'os-gradua{\c c}\~ao e Pesquisa de
  Engenharia (COPPE), toda abreviatura deve ser definida antes de
  utilizada.\abbrev{COPPE}{Instituto Alberto Luiz Coimbra de P\'os-gradua{\c
  c}\~ao e Pesquisa de Engenharia}

  Do mesmo modo, \'e imprescind\'ivel definir os s\'imbolos, tal como o
  conjunto dos n\'umeros reais $\mathbb{R}$ e o conjunto vazio $\emptyset$.
  \symbl{$\mathbb{R}$}{Conjunto dos n\'umeros reais}
  \symbl{$\emptyset$}{Conjunto vazio}

  \begin{longquote}
  Um exemplo de citação longa nas regras da ABNT (4cm de recuo e fonte menor)
  feita com o ambiente  \verb=longquote= The primary objective of this
  investigation was to determine the feasibility of detecting corrosion in
  aluminum Naval aircraft components with neutron radiographic interrogation
  and the use of standard corrosion penetrameters. Secondary objectives
  included the determination of the effect of object thickness on image quality,
  the defining of minimum levels of detectability and a preliminary investigation
  of a means whereby the degree of corrosion could be quantified with neutron
  radiographic data.
  \end{longquote}

  Para ilustrar a completa ades\~ao ao estilo de cita{\c c}\~oes e listagem de
  refer\^encias bibliogr\'aficas, a Tabela~\ref{tab:citation} apresenta cita{\c
  c}\~oes de alguns dos trabalhos contidos na norma fornecida pela CPGP da
  COPPE, utilizando o estilo num\'erico.

  \begin{table}[h]
  \caption{Exemplos de cita{\c c}\~oes utilizando o comando padr\~ao
    \texttt{\textbackslash cite} do \LaTeX\ e
    o comando \texttt{\textbackslash citet},
    fornecido pelo pacote \texttt{natbib}.}
  \label{tab:citation}
  \centering
  {\footnotesize
  \begin{tabular}{|c|c|c|}
    \hline
    Tipo da Publica{\c c}\~ao & \verb|\cite| & \verb|\citet|\\
    \hline
    Livro & \cite{book-example} & \citet{book-example}\\
    Artigo & \cite{article-example} & \citet{article-example}\\
    Relat\'orio & \cite{techreport-example} & \citet{techreport-example}\\
    Relat\'orio & \cite{techreport-exampleIn} & \citet{techreport-exampleIn}\\
    Anais de Congresso & \cite{inproceedings-example} &
      \citet{inproceedings-example}\\
    S\'eries & \cite{incollection-example} & \citet{incollection-example}\\
    Em Livro & \cite{inbook-example} & \citet{inbook-example}\\
    Disserta{\c c}\~ao de mestrado & \cite{mastersthesis-example} &
      \citet{mastersthesis-example}\\
    Tese de doutorado & \cite{phdthesis-example} & \citet{phdthesis-example}\\
    \hline
  \end{tabular}}
  \end{table}

  \backmatter
  \bibliographystyle{coppe-unsrt}
  \bibliography{example}

  \appendix
  \chapter{Algumas Demonstra{\c c}\~oes}
\end{document}
%% 
%%
%% End of file `example.tex'.
