%%
%% This is file `example.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% coppe.dtx  (with options: `example')
%% 
%% This is a sample monograph which illustrates the use of `coppe' document
%% class and `coppe-unsrt' BibTeX style.
%% 
%% \CheckSum{1613}
%% \CharacterTable
%%  {Upper-case    \A\B\C\D\E\F\G\H\I\J\K\L\M\N\O\P\Q\R\S\T\U\V\W\X\Y\Z
%%   Lower-case    \a\b\c\d\e\f\g\h\i\j\k\l\m\n\o\p\q\r\s\t\u\v\w\x\y\z
%%   Digits        \0\1\2\3\4\5\6\7\8\9
%%   Exclamation   \!     Double quote  \"     Hash (number) \#
%%   Dollar        \$     Percent       \%     Ampersand     \&
%%   Acute accent  \'     Left paren    \(     Right paren   \)
%%   Asterisk      \*     Plus          \+     Comma         \,
%%   Minus         \-     Point         \.     Solidus       \/
%%   Colon         \:     Semicolon     \;     Less than     \<
%%   Equals        \=     Greater than  \>     Question mark \?
%%   Commercial at \@     Left bracket  \[     Backslash     \\
%%   Right bracket \]     Circumflex    \^     Underscore    \_
%%   Grave accent  \`     Left brace    \{     Vertical bar  \|
%%   Right brace   \}     Tilde         \~}
%%
% useful link: https://apgita.org.br/academico/teses-e-latex/
\documentclass[msc,numbers,english]{coppe} % maybe mscexam?
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{xcolor}

\makelosymbols
\makeloabbreviations

\begin{document}
  \title{Tradução de linguagem automática em português brasileiro através de redes neurais em domínios de baixo recurso}
  \foreigntitle{Tackling low resource neural machine translation applied to Brazilian Portuguese}
  \author{Arthur}{Telles Estrella}
  \advisor{Prof.}{João}{Baptista de Oliveira e Souza Filho}{D.Sc.}
  % \advisor{Prof.}{Nome do Segundo Orientador}{Sobrenome}{Ph.D.}
  % \advisor{Prof.}{Nome do Terceiro Orientador}{Sobrenome}{D.Sc.}

  \examiner{Prof.}{Nome do Primeiro Examinador Sobrenome}{D.Sc.}
  \examiner{Prof.}{Nome do Segundo Examinador Sobrenome}{Ph.D.}
  \examiner{Prof.}{Nome do Terceiro Examinador Sobrenome}{D.Sc.}
  \examiner{Prof.}{Nome do Quarto Examinador Sobrenome}{Ph.D.}
  \examiner{Prof.}{Nome do Quinto Examinador Sobrenome}{Ph.D.}
  \department{PEE}
  \date{02}{2021}

  \keyword{Primeira palavra-chave}
  \keyword{Segunda palavra-chave}
  \keyword{Terceira palavra-chave}

  \maketitle

  \frontmatter
  \dedication{A algu\'em cujo valor \'e digno desta dedicat\'oria.}

  \chapter*{Agradecimentos}

  Gostaria de agradecer a todos.

  \begin{abstract}

  Apresenta-se nesta dissertação um estudo dedicado a lidar com a tarefa de tradução usando redes neurais, em condições de pouca disponibilidade de dados e com apenas uma GPU, com foco específico para o português-inglês. 
  Será avaliado o efeito prático de técnicas disponíveis na literatura que possuem algum potencial de melhorar a performance nesse contexto, como subword embeddings, pretrained word embeddings e back translation, e como elas  impactam qualitativamente no desempenho em frases de diferentes níveis de complexidade.
  Essas técnicas terão seus prós e contras avaliados e discutidos, utilizando as principais arquiteturas utilizadas na literatura, redes neurais recorrentes e baseadas em transformers.
  (avaliar isso) O melhor modelo desenvolvido é capaz de atingir x\% de score BLEU e y de perplexidade no conjunto de teste do dataset xpto.

  \end{abstract}

  \begin{foreignabstract}

  In this work, a dedicated analysis is executed to get best practices on how to tackle neural machine translation under low data availability and using a single GPU, focusing specifically on the Portuguese-English pair. 
  Techniques in the literature that can potentially boost the performance under this context will be evaluated, such as subword embeddings, pretrained word embeddings and back translation, and the qualitative impact of them is presented in sentences with a complexity drill down.
  The tradeoffs of these techniques are discussed, contemplating the main architectures used in the literature, neural recurrent models and transformer-based ones.
  (evaluate this) The best model built is capable of reaching x\% BLEU score and y Perplexity in the test set for the xpto Portuguese dataset.

  \end{foreignabstract}

  \tableofcontents
  \listoffigures
  \listoftables
  %% --- UNDERSTAND HOW TO USE THIS ---
  \printlosymbols
  \printloabbreviations
  %% ----------------------------------
  \mainmatter
  \chapter{Introduction}
  \section{The reenactment of machine translation}
  
  The machine translation is a research field that until 2013 has mainly invested in statistical based models, but the breakthrough promoted by sequence to sequence algorithms followed by the use of transformer models has significantly changed the focus of the field. Before neural networks, machine translation systems were rules-based, syntax-based, phrase-based or a blend between more than one of these techniques. Probabilistic models were used and considered state of the art just before the first sequence to sequence paper appeared. The increase in performance promoted by the sequence to sequence and transformers quickly received some attention, and soon other variants were developed.
  
  Despite being constrained by computational power in many stages since its beginning, one of the most relevant contributions to the translation task were the transformers, which made possible to perform the computation in a truly parallel schema. With this new architecture, the operations performed during training are not totally dependent, allowing them to become parallelizable in the GPU. By removing the constraint of some operations having to wait for others to finish, NMT models were enabled to scale and reach even higher quality translations.
  
  In 3 years NMT became the dominant approach to machine translation, inducing a major transition from statistical to neural models. The broad set of parameters and architectures already present in the literature that could be used to boost translation quality, along with the promising results being presented at the time reached the interest of researchers to explore this variant.

  \section{Challenges for the Portuguese language}

  Traditionally, the machine translation datasets and conferences usually focus on a subset of languages from countries that are actively investing on NLP, which biases and narrows the potential that the algorithms have towards a specific domain. Unfortunately, Portuguese is a language that does not ostentate supervised translation data in diversity and quantity, an issue that increases the struggle to build a model that can successfully translate it to other languages. Another obstacle is that Portuguese has european, brazilian and african variants, this provides a challenge for a model since generalization is harder if several sentences with different dialects can have the same meaning.
  
  The branch of NMT inside natural language processing is also a field with few papers and academic works among Brazilian universities, this can be partially explained by the challenge that this environment presents: most models require cutting edge GPUs and usually only one GPU is not enough for medium sized model on an average WMT competition dataset. The scarcity of these resources for research purposes require students to innovate in a limited domain and search for cloud solutions without sponsorship.
  
  Finally, Portuguese is a complex language that uses accents which can change their meaning (i.e. "e" and "é"), has different pronoun organizations (i.e. "realizar-se-á" equals "se realizará") and irregular verb inflections (i.e. the "pôr" and "haver" verbs) so text preprocessing and tokenization plays an important role. Disregarding these details by applying some generic preprocessing steps that eliminates accents for instance can lead to worse model performance. On the other hand, having some domain knowledge and apply this to the NLP pipeline can help the model better translate or classify, depending on the desired task.
  
  \section{Contributions of this dissertation}
  
  The generic contribution of this work is the evaluation of a set of techniques available in the literature for NMT that can help dealing with low resource domains, although being applied to Portuguese they can be generalized to other languages. There is also a specialized contribution under a qualitative domain, as performance gains provided by these techniques are evaluated in a subset of sentences that contain a complexity drill down, enabling the reader to have a deeper understanding of their effects on the Portuguese language.

  \section{Chapter Organization}

  \textcolor{red}{(a ideia final para a dissertação do chapter 2 é falar de arquitetura e revisão teórica/bibliográfica de fundamentos da arquitetura, depois no 3 mostrar restrições que acontecem em low resource domains e suas consequencias, com revisão bibliográfica das técnicas disponíveis que podem ajudar nesse contexto e a motivação das escolhas feitas)}

  An explanation of the transformer and recurrent neural network architectures and how researchers iterated on them to become state of the art is given in chapter 2.
  
  In chapter 3 constraints that arise in low resource domains and a review of techniques that can potentially help to reduce those issues are presented.
  
  The datasets chosen are presented in chapter 4, where we also describe implementation details, how hyperparameters were iterated on to fit in a single machine GPU and results of the experiments, which are measured in terms of validation perplexity, BLEU and \textcolor{red}{TER(?)(<to be defined>)}. A qualitative discussion regarding human and model translations in different complexity levels is also performed.
  
  \textcolor{red}{(Finally, in chapter 5 this work is concluded and further improvements and study directions are outlined.)}
  
  Finally, in chapter 5, next steps to conclude this work in the following months are presented

  \chapter{Neural networks and machine translation}

  Since their ideation in 1958, neural networks have seen peaks and valleys of research focus in many different artificial intelligence fields. After going out of the tar pit in the 1980s with feedforward and recurrent variants, receiving convolutional and LSTM variants in 1990s, they started to leverage promising results when applied to numerical and categorical data. Specially after 2013 neural networks were growing at an unseen rate, applications to image processing and natural language processing were massively explored and soon consolidated themselves as state of the art algorithms. 
  
  For a long time, this paradigm hasn't seen any applications to solve translation tasks, until 2013 when \citet{cho-etal-2014-learning} came up with a RNN Encoder-Decoder architecture proposal. The focus of the scientific community at the time that was on statistical models, but as the application of recurrent networks to this domain gained maturity, the papers gradually started to switch their focus.
  
  \section{Recurrent neural networks as machine translators}

  In Cho et al's approach, one RNN reads each symbol of an input sequence x sequentially and encodes this sequence of symbols into a fixed-length vector representation $c$, the hidden state. The other decodes this representation into another sequence of symbols by predicting the next symbol $y_{t}$ given the hidden state $h_{{t}}$. The equation for the decoder's hidden state at time $t$ is given below.

  \begin{equation}
  h_{{t}} = f(h_{t-1}, y_{t-1}, c)
  \end{equation}
  
  In this setup, the encoder and decoder of the proposed model are jointly trained to maximize the conditional log-likelihood of a target sequence given a source sequence. It has the drawback of losing some of the information provided by the encoder, since only the hidden state is used, and the RNN output is discarded. Another issue is that the neural network has to compress all the necessary information of a source sentence into a fixed-length vector, which provides a challenge to deal with long sentences.
  
  Massive Exploration of Neural Machine Translation Architectures \citet{britz2017massive}

  \subsection{Bahdanau's attention mechanism}

  NMT by jointly learning to align and translate, comment that it has a new sequence proposal for vectorization
  
  \citet{bahdanau2016neural}
  
  \subsection{Attention variants for sequence to sequence learning}
  
  Effective Approaches to Attention-based Neural Machine Translation \citet{luong-etal-2015-effective}
  
  RNN with Doubly-Attentive Decoder \citet{calixto-etal-2017-doubly}

  \section{Transformer-based models}

  Attention is all you need \citet{DBLP:journals/corr/VaswaniSPUJGKP17}

  \subsection{Have transformers outperformed RNNs?}
  
  A Comparison of Transformer and Recurrent Neural Networks on Multilingual
               Neural Machine Translation
  \citet{DBLP:journals/corr/abs-1806-06957}

  Talk briefly of GPT, maybe bert, and other transformer variations, including using word embeddings 
  
  GPT-3 \citet{brown2020language}
  
  DeFINE: DEep Factorized INput Word Embeddings for Neural Sequence \citet{DBLP:journals/corr/abs-1911-12385}
  
  DeLighT: Deep and Light-weight Transformer \citet{mehta2021delight}
  
  \subsection{training techniques can also help}
  
  Minimum Risk Training for Neural Machine Translation \citet{shen-etal-2016-minimum}

  \chapter{Low resource context}
  

  \section{low resource techniques}
  
  Transfer Learning for Low-Resource Neural Machine Translation \citet{zoph-etal-2016-transfer}
  
  Revisiting Low-Resource Neural Machine Translation: A Case Study \citet{sennrich-zhang-2019-revisiting}
  
  Talk of using Glove, word2vec, fasttext and others
  
 

  \chapter{Results}

  ------- (step by step explanation of the hyperparameters explored, results and building piece on piece until we reach out best model) -------

  a

  Segundo a norma de formata{\c c}\~ao de teses e disserta{\c c}\~oes do
  Instituto Alberto Luiz Coimbra de P\'os-gradua{\c c}\~ao e Pesquisa de
  Engenharia (COPPE), toda abreviatura deve ser definida antes de
  utilizada.\abbrev{COPPE}{Instituto Alberto Luiz Coimbra de P\'os-gradua{\c
  c}\~ao e Pesquisa de Engenharia}

  Do mesmo modo, \'e imprescind\'ivel definir os s\'imbolos, tal como o
  conjunto dos n\'umeros reais $\mathbb{R}$ e o conjunto vazio $\emptyset$.
  \symbl{$\mathbb{R}$}{Conjunto dos n\'umeros reais}
  \symbl{$\emptyset$}{Conjunto vazio}

  \begin{longquote}
  Um exemplo de citação longa nas regras da ABNT (4cm de recuo e fonte menor)
  feita com o ambiente  \verb=longquote= The primary objective of this
  investigation was to determine the feasibility of detecting corrosion in
  aluminum Naval aircraft components with neutron radiographic interrogation
  and the use of standard corrosion penetrameters. Secondary objectives
  included the determination of the effect of object thickness on image quality,
  the defining of minimum levels of detectability and a preliminary investigation
  of a means whereby the degree of corrosion could be quantified with neutron
  radiographic data.
  \end{longquote}

  Para ilustrar a completa ades\~ao ao estilo de cita{\c c}\~oes e listagem de
  refer\^encias bibliogr\'aficas, a Tabela~\ref{tab:citation} apresenta cita{\c
  c}\~oes de alguns dos trabalhos contidos na norma fornecida pela CPGP da
  COPPE, utilizando o estilo num\'erico.

  \begin{table}[h]
  \caption{Exemplos de cita{\c c}\~oes utilizando o comando padr\~ao
    \texttt{\textbackslash cite} do \LaTeX\ e
    o comando \texttt{\textbackslash citet},
    fornecido pelo pacote \texttt{natbib}.}
  \label{tab:citation}
  \centering
  {\footnotesize
  \begin{tabular}{|c|c|c|}
    \hline
    Tipo da Publica{\c c}\~ao & \verb|\cite| & \verb|\citet|\\
    \hline
    Livro & \cite{book-example} & \citet{book-example}\\
    Artigo & \cite{article-example} & \citet{article-example}\\
    Relat\'orio & \cite{techreport-example} & \citet{techreport-example}\\
    Relat\'orio & \cite{techreport-exampleIn} & \citet{techreport-exampleIn}\\
    Anais de Congresso & \cite{inproceedings-example} &
      \citet{inproceedings-example}\\
    S\'eries & \cite{incollection-example} & \citet{incollection-example}\\
    Em Livro & \cite{inbook-example} & \citet{inbook-example}\\
    Disserta{\c c}\~ao de mestrado & \cite{mastersthesis-example} &
      \citet{mastersthesis-example}\\
    Tese de doutorado & \cite{phdthesis-example} & \citet{phdthesis-example}\\
    \hline
  \end{tabular}}
  \end{table}

  \chapter{Results}
  
  \section{Datasets}
  
  Apart from the issues outlined, there is a collaborative dataset created and maintained by the Tatoeba Project that contains several English-Portuguese sentence pairs, that is available on the internet. The project contains datasets with several bilingual sentence pairs, considered to match an intermediate english level and translated from English to several other languages, including Portuguese. The Portuguese dataset contains 165k sentences, with 993k words and 21k unique words. It was built by native speakers and has been reviewed by a linguistic expert.

  When compared to the average WMT datasets, it is not as large and the vocabulary size is a bit smaller, so it is considered medium level dataset to machine translation, but it suffices our needs.
  
  \chapter{Conclusions}

  \backmatter
  \bibliographystyle{coppe-unsrt}
  \bibliography{example}

  \appendix
  \chapter{Algumas Demonstra{\c c}\~oes}
\end{document}
%% 
%%
%% End of file `example.tex'.
